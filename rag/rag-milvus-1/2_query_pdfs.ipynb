{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG With llama-index  + Milvus + Qwen - Part 2\n",
    "\n",
    "References\n",
    "\n",
    "- https://studio.nebius.com/\n",
    "- https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/\n",
    "- https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/milvus/?h=milvusvectorstore#llama_index.vector_stores.milvus.MilvusVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found NEBIUS_API_KEY in environment, using it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv('NEBIUS_API_KEY'):\n",
    "    print (\"✅ Found NEBIUS_API_KEY in environment, using it\")\n",
    "else:\n",
    "    raise ValueError(\"❌ NEBIUS_API_KEY not found in environment. Please set it in .env file before running this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Setup Embedding Model\n",
    "\n",
    "We have a choice of local embedding model (fast) or running it on the cloud\n",
    "\n",
    "If running locally:\n",
    "- choose smaller models\n",
    "- less accuracy but faster\n",
    "\n",
    "If running on the cloud\n",
    "- We can run large models (billions of params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Option 1: Running embedding models on Nebius cloud\n",
    "from llama_index.embeddings.nebius import NebiusEmbedding\n",
    "EMBEDDING_MODEL = 'Qwen/Qwen3-Embedding-8B'  # 8B params\n",
    "EMBEDDING_LENGTH = 4096  # Length of the embedding vector\n",
    "Settings.embed_model = NebiusEmbedding(\n",
    "                        model_name=EMBEDDING_MODEL,\n",
    "                        embed_batch_size=50,  # Batch size for embedding (default is 10)\n",
    "                        api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified here, it will get taken from env variable\n",
    "                       )\n",
    "\n",
    "## Option 2: Running embedding models locally\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     # model_name = 'sentence-transformers/all-MiniLM-L6-v2' # 23 M params\n",
    "#     model_name = 'BAAI/bge-small-en-v1.5'  # 33M params\n",
    "#     # model_name = 'Qwen/Qwen3-Embedding-0.6B'  # 600M params\n",
    "#     # model_name = 'BAAI/bge-en-icl'  # 7B params\n",
    "#     #model_name = 'intfloat/multilingual-e5-large-instruct'  # 560M params\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at schema.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at common.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at milvus.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at rg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at feder.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.31.1 at msg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance:  ./rag.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "DB_URI = './rag.db'  # For embedded instance\n",
    "COLLECTION_NAME = 'rag'\n",
    "\n",
    "milvus_client = MilvusClient(DB_URI)\n",
    "print (\"✅ Connected to Milvus instance: \", DB_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected Llama-index to Milvus instance:  ./rag.db\n"
     ]
    }
   ],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = DB_URI ,\n",
    "    dim = EMBEDDING_LENGTH ,\n",
    "    collection_name = COLLECTION_NAME,\n",
    "    overwrite=False\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"✅ Connected Llama-index to Milvus instance: \", DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Load Document Index from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded index from vector db: ./rag.db\n",
      "CPU times: user 94 ms, sys: 20.7 ms, total: 115 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context)\n",
    "\n",
    "print (\"✅ Loaded index from vector db:\", DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/my-stuff/projects/nebius/ai-studio-cookbook-1/rag/rag-milvus-1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.nebius import NebiusLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = NebiusLLM(\n",
    "                model='openai/gpt-oss-120b',\n",
    "                # model='Qwen/Qwen3-30B-A3B',\n",
    "                # model='deepseek-ai/DeepSeek-R1-0528',\n",
    "                api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified, it will get taken from env variable\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber’s revenue for 2020 was $11.139 billion.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What was Uber's revenue for 2020?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure the model uses context\n",
    "\n",
    "Let's ask a generic factual question \"When was the moon landing\".\n",
    "\n",
    "Now the model should know this generic factual answer.\n",
    "\n",
    "But since we are querying documents, we want to the model to find answers from within the documents.\n",
    "\n",
    "It should come back with something like \"provided context does not have information about moon landing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain information about the date of the moon landing.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"When was the moon landing?\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-milvus-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
